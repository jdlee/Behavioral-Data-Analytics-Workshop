---
title: "Overview of Machine Learning: Opportunities and Challenges"
author: "John D. Lee and Linda Ng Boyle"
date: 10/28/2019
output:
  ioslides_presentation:
    widescreen: yes
  beamer_presentation: default
---

```{r setup, include=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)

library(tidyverse)
library(skimr)
library(lme4)

set.seed(1)
rm(list = ls(all = TRUE))
```


## HF Considerations for ML and <br> ML Considerations for HF
-- Human-centered ML represents a looming societal need

-- ML deployment sometimes neglects design and systems thinking

-- ML mindset for data analysis might save us from the p-value ritual

-- ML and Human-centered tradeoffs in model development (e.g., understanding and fairness)



## Increasing Centrality of Machine Learning in System Design|Human implications of increasingly powerful algorithms

```{r, out.width="33%"}
knitr::include_graphics("Weapons_of_Math_Destruction.jpg", dpi = 220)
```



## Design Thinking and Systems Thinking for ML{.smaller}

-- Design thinking: Empathy and understanding for person-centered solutions

-- Systems thinking: 5 whys? 5 whats?

-- Sea-level and C-level support for success


```{r UCE, echo = FALSE, warning=FALSE}
knitr::include_graphics("UCE.jpg", dpi = 220) #320 for pdf

```

<span style="font-size:.75em">
Lee, J. D., Wickens, C. D., Liu, Y., & Boyle, L. N. (2017). Designing for People: An introduction to human factors engineering. Charleston, SC: CreateSpace.
</span>





## Machine Learning versus <br> Inferential Data Analysis{.bigger}

-- Linear regression and logistic regression as machine learning?

ML mindset| Traditional inferential mindset
-----|-----:
Prediction|Inference
Cross validation and prediction error|Hypothesis testing and p values
Practical significance|Statistical significance



## Holdout validation and prediction error

```{r fitvalidation, echo = FALSE, warning=FALSE,fig.height=7}
knitr::include_graphics("1_FittingValidation.png", dpi=120) #220 for pdf
```



## Types of Machine Learning
-- Supervised learning: Predicting a known output

-- Unsupervised learning: Identifying unknown patterns or clusters

-- Reinforcement learning: Learning through interactions with a system

```{r typesofML, echo = FALSE, warning=FALSE}
knitr::include_graphics("SupervisedUnsupervisedReinforcement.png", dpi = 90) #190 for pdf
```
<span style="font-size:.8em">
https://www.ibm.com/developerworks/library/cc-models-machine-learning/index.html
</span>



## Supervised Learning: More than curve fitting?
```{r curvefitting, echo = FALSE, warning=FALSE}
knitr::include_graphics("curve_fitting_cartoon.png", dpi = 175) #275 for pdf
```



## Essential Tradeoffs in ML Design|Variance-bias tradeoff of a model $\theta${.smaller}
$\Omega(\theta)=$ Model complexity, ideally small variance and good generalization

$L(\theta) =  \sum_{i=1}^n (y_i-\hat{y_i})^2 =$ Model error, ideally small bias and precise predictions

Objective function$(\theta) = \Omega(\theta) + L(\theta)$

```{r tradeoffsofML, echo = FALSE, warning=FALSE}
knitr::include_graphics("VarianceBias.png", dpi = 180) # 300 for pdf
```

https://xgboost.readthedocs.io/en/latest/index.html


## Essential Tradeoffs in ML Design| Human-centered tradeoffs
-- Different errors: Cost of a miss may differ than a cost of a false alarm

$L(\theta) =  \sum_{i=1}^n (y_i-\hat{y_i})^2$ Positive errors equally problematic as negative errors??
<br />
<br />

-- Trust depends on more than AUC (Area Under the Receiver-Operator Curve): Hard hits don't compensate for easy misses

-- People might value understandable models more than precise models

$\Omega(\theta)$ Model complexity does not typically reflect perceived complexity 



## Essential Tradeoffs in ML Design| Hyperparameters{.build}

Hyperparameters (as in $\lambda$ below) adjust algorithms to promote:

-- A better model fit

-- A more robust model

-- A more understandable model

Standard regression: Cost function = $RSS = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2$

Ridge regression: Cost functon $= \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda \sum_{j=1}^p\beta_j^2$

<span style="font-size:.75em">
James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. New York: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7
</span>





## HF Considerations for ML and <br> ML Considerations for HF{.build}
-- Human-centered ML represents a looming societal need

-- Design and systems thinking is sometimes neglected with ML deployment

-- ML mindset for data analysis might save us from the p-value ritual

-- ML and Human-centered tradeoffs in model development

